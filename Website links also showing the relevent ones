# imports
# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt

import os
import requests
import json
from typing import List
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from IPython.display import Markdown, display, update_display
from openai import OpenAI
import google.generativeai as genai
# --- Initialize and constants ---

# load_dotenv(override=True)
# api_key = os.getenv('GOOGLE_API_KEY')  # Make sure your .env file contains GOOGLE_API_KEY=
api_key = "AIzaSyCk03Hu69ksdYVfZTuhRbM-VI8mVp2aDsA"

if api_key and len(api_key) > 20:
    print("API key looks good so far")
else:
    print("There might be a problem with your Gemini API key? Please check your .env file or Google Cloud console!")

# Configure Gemini
genai.configure(api_key=api_key)

# Choose model
MODEL = "gemini-2.5-flash"  # or "gemini-1.5-pro" for higher quality

# --- Example: Basic test call ---
model = genai.GenerativeModel(MODEL)

prompt = "Write a short motivational message for a data scientist."

response = model.generate_content(prompt)

# Display response nicely
display(Markdown(response.text))
# A class to represent a Webpage

# Some websites need you to use proper headers when fetching them:
headers = {
 "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
}

class Website:
    """
    A utility class to represent a Website that we have scraped, now with links
    """

    def __init__(self, url):
        self.url = url
        response = requests.get(url, headers=headers)
        self.body = response.content
        soup = BeautifulSoup(self.body, 'html.parser')
        self.title = soup.title.string if soup.title else "No title found"
        if soup.body:
            for irrelevant in soup.body(["script", "style", "img", "input"]):
                irrelevant.decompose()
            self.text = soup.body.get_text(separator="\n", strip=True)
        else:
            self.text = ""
        links = [link.get('href') for link in soup.find_all('a')]
        self.links = [link for link in links if link]

    def get_contents(self):
        return f"Webpage Title:\n{self.title}\nWebpage Contents:\n{self.text}\n\n"
ed = Website("https://edwarddonner.com")
ed.links
link_system_prompt = "You are provided with a list of links found on a webpage. \
You are able to decide which of the links would be most relevant to include in a brochure about the company, \
such as links to an About page, or a Company page, or Careers/Jobs pages.\n"
link_system_prompt += "You should respond in JSON as in this example:"
link_system_prompt += """
{
    "links": [
        {"type": "about page", "url": "https://full.url/goes/here/about"},
        {"type": "careers page", "url": "https://another.full.url/careers"}
    ]
}
"""
print(link_system_prompt)
def get_links_user_prompt(website):
    user_prompt = f"Here is the list of links on the website of {website.url} - "
    user_prompt += "please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \
Do not include Terms of Service, Privacy, email links.\n"
    user_prompt += "Links (some might be relative links):\n"
    user_prompt += "\n".join(website.links)
    return user_prompt
print(get_links_user_prompt(ed))
# def get_links(url):
#     website = Website(url)
#     response = openai.chat.completions.create(
#         model=MODEL,
#         messages=[
#             {"role": "system", "content": link_system_prompt},
#             {"role": "user", "content": get_links_user_prompt(website)}
#       ],
#         response_format={"type": "json_object"}
#     )
#     result = response.choices[0].message.content
#     return json.loads(result)
def get_links(url):
    website = Website(url)

    # Combine the system and user prompts into one text prompt
    full_prompt = f"""
    {link_system_prompt}

    User input:
    {get_links_user_prompt(website)}
    """

    # Send the prompt to Gemini
    response = model.generate_content(
        full_prompt,
        generation_config={"response_mime_type": "application/json"}  # ensures Gemini returns JSON
    )

    # Parse and return the JSON safely
    try:
        result = json.loads(response.text)
        return result
    except json.JSONDecodeError:
        print("⚠️ Gemini response was not valid JSON:")
        print(response.text)
        return None
# Anthropic has made their site harder to scrape, so I'm using HuggingFace..

huggingface = Website("https://huggingface.co")
huggingface.links
get_links("https://huggingface.co")
